{
  "uid" : "eb6f5b5133a6168e",
  "name" : "test_xfail_not_run",
  "fullName" : "tests.test_marks.TestMarks#test_xfail_not_run",
  "historyId" : "e4c8f58bb9381dbcdd35cf7a17339a5f",
  "time" : {
    "start" : 1589120411564,
    "stop" : 1589120411564,
    "duration" : 0
  },
  "description" : "run=false 表示不执行这条用例",
  "descriptionHtml" : "<p>run=false 表示不执行这条用例</p>\n",
  "status" : "skipped",
  "statusMessage" : "_pytest.outcomes.XFailed: [NOTRUN]",
  "statusTrace" : "cls = <class '_pytest.runner.CallInfo'>, func = <function call_runtest_hook.<locals>.<lambda> at 0x00000210BD9573A0>\nwhen = 'setup', reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)\n\n    @classmethod\n    def from_call(cls, func, when, reraise=None) -> \"CallInfo\":\n        #: context of invocation: one of \"setup\", \"call\",\n        #: \"teardown\", \"memocollect\"\n        start = time()\n        excinfo = None\n        try:\n>           result = func()\n\nd:\\python\\python38\\lib\\site-packages\\_pytest\\runner.py:244: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nd:\\python\\python38\\lib\\site-packages\\_pytest\\runner.py:217: in <lambda>\n    lambda: ihook(item=item, **kwds), when=when, reraise=reraise\nd:\\python\\python38\\lib\\site-packages\\pluggy\\hooks.py:286: in __call__\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\nd:\\python\\python38\\lib\\site-packages\\pluggy\\manager.py:93: in _hookexec\n    return self._inner_hookexec(hook, methods, kwargs)\nd:\\python\\python38\\lib\\site-packages\\pluggy\\manager.py:84: in <lambda>\n    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\nd:\\python\\python38\\lib\\site-packages\\_pytest\\skipping.py:93: in pytest_runtest_setup\n    check_xfail_no_run(item)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nitem = <Function test_xfail_not_run>\n\n    def check_xfail_no_run(item):\n        \"\"\"check xfail(run=False)\"\"\"\n        if not item.config.option.runxfail:\n            evalxfail = item._store[evalxfail_key]\n            if evalxfail.istrue():\n                if not evalxfail.get(\"run\", True):\n>                   xfail(\"[NOTRUN] \" + evalxfail.getexplanation())\nE                   _pytest.outcomes.XFailed: [NOTRUN]\n\nd:\\python\\python38\\lib\\site-packages\\_pytest\\skipping.py:111: XFailed",
  "flaky" : false,
  "beforeStages" : [ ],
  "afterStages" : [ ],
  "labels" : [ {
    "name" : "parentSuite",
    "value" : "tests"
  }, {
    "name" : "suite",
    "value" : "test_marks"
  }, {
    "name" : "subSuite",
    "value" : "TestMarks"
  }, {
    "name" : "host",
    "value" : "8S9K3DZP3JRJTZY"
  }, {
    "name" : "thread",
    "value" : "17372-MainThread"
  }, {
    "name" : "framework",
    "value" : "pytest"
  }, {
    "name" : "language",
    "value" : "cpython3"
  }, {
    "name" : "package",
    "value" : "tests.test_marks"
  }, {
    "name" : "resultFormat",
    "value" : "allure2"
  } ],
  "parameters" : [ ],
  "links" : [ ],
  "hidden" : true,
  "retry" : true,
  "extra" : {
    "categories" : [ ],
    "tags" : [ ]
  },
  "source" : "eb6f5b5133a6168e.json",
  "parameterValues" : [ ]
}